{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kZIJkVCognv6MwvZk_Z5SNQD0iuw9NVf",
      "authorship_tag": "ABX9TyOvF0Q18jPFX1tSwoZo3Z+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeongmin0658/kentech_EF_CCP/blob/main/codes/particle_based/ML/VAE/VAE_V0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP88Sobe3YTB",
        "outputId": "72737b23-a2e8-4eec-9808-78afaac57e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets,transforms\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "WhjM0kDik_Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = './drive/MyDrive/DLA_Datas/img_data'\n",
        "test_path = './drive/MyDrive/DLA_Datas/test_data'\n",
        "result_image_path = './drive/MyDrive/DLA_Datas/result_img'\n",
        "channels = 3                   # MNIST has only 1\n",
        "\n",
        "n_epochs = 30\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "b1 = 0.3\n",
        "b2 = 0.999\n",
        "\n",
        "img_size = 200\n",
        "hidden_dim = 400\n",
        "latent_dim = 10"
      ],
      "metadata": {
        "id": "n4QdJgNWlB-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from PIL import Image\n",
        "cnt = 0\n",
        "#fig, axes = plt.subplots(1, 20, figsize=(15, 5))\n",
        "ha_data = []\n",
        "x = 0\n",
        "img = []\n",
        "for ax in range(721):\n",
        "  readcsv = []\n",
        "  with open(f\"./drive/MyDrive/DLA_Datas/data/DLA1_{cnt}.csv\",\"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      readcsv.append(list(map(int,row)))\n",
        "    f.close()\n",
        "  ha_data = np.array(readcsv)\n",
        "  #ha_data = ha_data.astype(bool)\n",
        "  size = ha_data.shape[::-1]\n",
        "  databytes = np.packbits(ha_data, axis=1)\n",
        "  img = Image.frombytes(mode='1', size=size, data=databytes)\n",
        "  #for i in ha_data:\n",
        "    #for j in i:\n",
        "      #print(j*255, end = \" \")\n",
        "    #print(\"\")\n",
        "  # display(img)\n",
        "  x = np.array(img)\n",
        "  img.save(f'./drive/MyDrive/DLA_Datas/img_data/output{cnt}.jpg', 'JPEG')\n",
        "\n",
        "  #img.save(\"DLA1_{cnt}.jpg\",\"JPEG\")\n",
        "  #ax.imshow(readcsv, cmap='gray')\n",
        "  #ax.axis('off')\n",
        "  cnt+=1\n",
        "\n",
        "#plt.tight_layout()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "U9NEWEmErSEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf5cf56-8719-41dc-94c2-c31e40791539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 52 104 190]\n",
            "  [ 56 108 194]\n",
            "  [ 53 105 191]\n",
            "  ...\n",
            "  [ 38  88 173]\n",
            "  [ 39  90 173]\n",
            "  [ 37  89 172]]\n",
            "\n",
            " [[ 52 104 190]\n",
            "  [ 53 105 191]\n",
            "  [ 56 108 194]\n",
            "  ...\n",
            "  [ 41  89 173]\n",
            "  [ 38  89 172]\n",
            "  [ 38  90 173]]\n",
            "\n",
            " [[ 51 106 189]\n",
            "  [ 49 103 189]\n",
            "  [ 52 104 190]\n",
            "  ...\n",
            "  [ 40  88 172]\n",
            "  [ 38  89 172]\n",
            "  [ 38  89 170]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 49  93 180]\n",
            "  [ 49  95 181]\n",
            "  [ 48  94 180]\n",
            "  ...\n",
            "  [ 35  81 169]\n",
            "  [ 36  80 169]\n",
            "  [ 36  80 169]]\n",
            "\n",
            " [[ 48  94 180]\n",
            "  [ 47  93 179]\n",
            "  [ 50  96 182]\n",
            "  ...\n",
            "  [ 35  81 169]\n",
            "  [ 37  81 170]\n",
            "  [ 36  80 169]]\n",
            "\n",
            " [[ 51  97 183]\n",
            "  [ 48  94 180]\n",
            "  [ 49  95 181]\n",
            "  ...\n",
            "  [ 37  81 168]\n",
            "  [ 40  81 171]\n",
            "  [ 39  80 170]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from PIL import Image\n",
        "cnt = 0\n",
        "#fig, axes = plt.subplots(1, 20, figsize=(15, 5))\n",
        "ha_data = []\n",
        "x = 0\n",
        "img = []\n",
        "for ax in range(260):\n",
        "  readcsv = []\n",
        "  with open(f\"./drive/MyDrive/DLA_Datas/data_0/DLA0_{cnt}.csv\",\"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      readcsv.append(list(map(int,row)))\n",
        "    f.close()\n",
        "  ha_data = np.array(readcsv)\n",
        "  #ha_data = ha_data.astype(bool)\n",
        "  size = ha_data.shape[::-1]\n",
        "  databytes = np.packbits(ha_data, axis=1)\n",
        "  img = Image.frombytes(mode='1', size=size, data=databytes)\n",
        "  #for i in ha_data:\n",
        "    #for j in i:\n",
        "      #print(j*255, end = \" \")\n",
        "    #print(\"\")\n",
        "  # display(img)\n",
        "  x = np.array(img)\n",
        "  img.save(f'./drive/MyDrive/DLA_Datas/img_data_0/output{cnt}.jpg', 'JPEG')\n",
        "\n",
        "  #img.save(\"DLA1_{cnt}.jpg\",\"JPEG\")\n",
        "  #ax.imshow(readcsv, cmap='gray')\n",
        "  #ax.axis('off')\n",
        "  cnt+=1\n",
        "\n",
        "#plt.tight_layout()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "tEPxiXcTurlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.array(Image.open(f'./drive/MyDrive/DLA_Datas/img_data/output1.jpg'))\n",
        "print(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17I4vGVZqFY0",
        "outputId": "e00c0a20-da9d-4697-e129-4bfacdaed1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "b1 = 0.5\n",
        "b2 = 0.999"
      ],
      "metadata": {
        "id": "tLNSb_Cbn-Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class DLA_Dataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_path_list = os.listdir(data_dir)\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = os.path.join(self.data_dir, self.image_path_list[index])\n",
        "        x_data = Image.open(image_path)\n",
        "        x_data = self.transform(x_data)\n",
        "\n",
        "        return x_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path_list)"
      ],
      "metadata": {
        "id": "oR0glWbuwphS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "a = DLA_Dataset(image_path)\n",
        "b = DLA_Dataset(test_path)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "            a,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "\n",
        ")\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "            b,\n",
        "            batch_size=128,\n",
        "            shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "SIUnmzp4yHUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(image_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "-yXStYoqmn8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMoVMUCamrMw",
        "outputId": "9c4fc76f-1e9f-4140-ca59-9640d4daa6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar/2)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std"
      ],
      "metadata": {
        "id": "3MQrC9BPnNyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, x_dim=img_size**2, h_dim=hidden_dim, z_dim=latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # 1st hidden layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(x_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "\n",
        "        # 2nd hidden layer\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "\n",
        "        # output layer\n",
        "        self.mu = nn.Linear(h_dim, z_dim)\n",
        "        self.logvar = nn.Linear(h_dim, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc2(self.fc1(x))\n",
        "\n",
        "        mu = F.relu(self.mu(x))\n",
        "        logvar = F.relu(self.logvar(x))\n",
        "\n",
        "        z = reparameterization(mu, logvar)\n",
        "        return z, mu, logvar"
      ],
      "metadata": {
        "id": "XdO2rXXipsMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, x_dim=img_size**2, h_dim=hidden_dim, z_dim=latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # 1st hidden layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(z_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "        )\n",
        "\n",
        "        # 2nd hidden layer\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "\n",
        "        # output layer\n",
        "        self.fc3 = nn.Linear(h_dim, x_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.fc2(self.fc1(z))\n",
        "        x_reconst = F.sigmoid(self.fc3(z))\n",
        "        return x_reconst"
      ],
      "metadata": {
        "id": "MSNXnnNsqMTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(encoder.parameters(), decoder.parameters()), lr=lr, betas=(b1, b2)\n",
        ")"
      ],
      "metadata": {
        "id": "l39T7QGPrlx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYhOnxWTrn8q",
        "outputId": "9a822e06-cca9-4ad5-9def-fca391bf6464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=40000, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (mu): Linear(in_features=400, out_features=10, bias=True)\n",
            "  (logvar): Linear(in_features=400, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3W54yV5rpDh",
        "outputId": "478141ff-fc83-419b-9b3d-ca2e2e0c0f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder(\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (fc3): Linear(in_features=400, out_features=40000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0\n",
        "    for i, (x) in enumerate(train_dataloader):\n",
        "        # forward\n",
        "        x = x.view(-1, img_size**2)\n",
        "        x = x.to(device)\n",
        "        z, mu, logvar = encoder(x)\n",
        "        x_reconst = decoder(z)\n",
        "\n",
        "        # compute reconstruction loss and KL divergence\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "        kl_div = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1)\n",
        "\n",
        "        # backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 3 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{len(train_dataloader)}], Reconst Loss : {reconst_loss.item():.4f}, KL Div: {kl_div.item():.4f}')\n",
        "\n",
        "    print(f'===> Epoch: {epoch+1} Average Train Loss: {train_loss/len(train_dataloader.dataset):.4f} ')\n",
        "\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (x) in enumerate(test_dataloader):\n",
        "            # forward\n",
        "            x = x.view(-1, img_size**2)\n",
        "            x = x.to(device)\n",
        "            z, mu, logvar = encoder(x)\n",
        "            x_reconst = decoder(z)\n",
        "\n",
        "            # compute reconstruction loss and KL divergence\n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            kl_div = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1)\n",
        "\n",
        "            loss = reconst_loss + kl_div\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # save reconstruction images\n",
        "            if i==0:\n",
        "                x_concat = torch.cat([x.view(-1, 1, 200, 200), x_reconst.view(-1, 1, 200, 200)], dim=3)\n",
        "                # batch size 개수만큼의 이미지 쌍(input x, reconstructed x)이 저장됨\n",
        "                save_image(x_concat, os.path.join(result_image_path,f'reconst-epoch{epoch+1}.png'))\n",
        "\n",
        "        print(f'===> Epoch: {epoch+1} Average Test Loss: {test_loss/len(test_dataloader.dataset):.4f} ')\n",
        "\n",
        "        # save sampled images\n",
        "        z = torch.randn(batch_size, latent_dim).to(device) # N(0, 1)에서 z 샘플링\n",
        "        sampled_images = decoder(z)\n",
        "        save_image(sampled_images.view(-1, 1, 200, 200), os.path.join(result_image_path,f'sampled-epoch{epoch+1}.png'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxfpG_ghrrEO",
        "outputId": "76eb599b-faef-4ef1-a4d8-b35c87fbad07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Epoch: 1 Average Train Loss: 27809.1355 \n",
            "===> Epoch: 1 Average Test Loss: 27718.5884 \n",
            "===> Epoch: 2 Average Train Loss: 27714.8863 \n",
            "===> Epoch: 2 Average Test Loss: 27634.7845 \n",
            "===> Epoch: 3 Average Train Loss: 27631.9632 \n",
            "===> Epoch: 3 Average Test Loss: 27541.0216 \n",
            "===> Epoch: 4 Average Train Loss: 27542.7843 \n",
            "===> Epoch: 4 Average Test Loss: 27437.7651 \n",
            "===> Epoch: 5 Average Train Loss: 27449.6639 \n",
            "===> Epoch: 5 Average Test Loss: 27349.3319 \n",
            "===> Epoch: 6 Average Train Loss: 27351.6438 \n",
            "===> Epoch: 6 Average Test Loss: 27220.6487 \n",
            "===> Epoch: 7 Average Train Loss: 27227.8796 \n",
            "===> Epoch: 7 Average Test Loss: 27061.1897 \n",
            "===> Epoch: 8 Average Train Loss: 27077.7140 \n",
            "===> Epoch: 8 Average Test Loss: 26946.6509 \n",
            "===> Epoch: 9 Average Train Loss: 26891.3662 \n",
            "===> Epoch: 9 Average Test Loss: 26642.8103 \n",
            "===> Epoch: 10 Average Train Loss: 26660.9365 \n",
            "===> Epoch: 10 Average Test Loss: 26372.4052 \n",
            "===> Epoch: 11 Average Train Loss: 26374.7926 \n",
            "===> Epoch: 11 Average Test Loss: 25998.0603 \n",
            "===> Epoch: 12 Average Train Loss: 25996.3629 \n",
            "===> Epoch: 12 Average Test Loss: 25470.9138 \n",
            "===> Epoch: 13 Average Train Loss: 25475.7642 \n",
            "===> Epoch: 13 Average Test Loss: 24814.1983 \n",
            "===> Epoch: 14 Average Train Loss: 24646.8027 \n",
            "===> Epoch: 14 Average Test Loss: 23651.3815 \n",
            "===> Epoch: 15 Average Train Loss: 23729.2876 \n",
            "===> Epoch: 15 Average Test Loss: 22489.2716 \n",
            "===> Epoch: 16 Average Train Loss: 22229.5284 \n",
            "===> Epoch: 16 Average Test Loss: 21064.8599 \n",
            "===> Epoch: 17 Average Train Loss: 20674.2441 \n",
            "===> Epoch: 17 Average Test Loss: 18875.8168 \n",
            "===> Epoch: 18 Average Train Loss: 18709.0853 \n",
            "===> Epoch: 18 Average Test Loss: 16537.5065 \n",
            "===> Epoch: 19 Average Train Loss: 16541.3913 \n",
            "===> Epoch: 19 Average Test Loss: 14274.0140 \n",
            "===> Epoch: 20 Average Train Loss: 14376.6806 \n",
            "===> Epoch: 20 Average Test Loss: 12594.4612 \n",
            "===> Epoch: 21 Average Train Loss: 12655.8135 \n",
            "===> Epoch: 21 Average Test Loss: 11225.7759 \n",
            "===> Epoch: 22 Average Train Loss: 11384.5242 \n",
            "===> Epoch: 22 Average Test Loss: 10133.3825 \n",
            "===> Epoch: 23 Average Train Loss: 10025.3353 \n",
            "===> Epoch: 23 Average Test Loss: 9306.1595 \n",
            "===> Epoch: 24 Average Train Loss: 9050.0460 \n",
            "===> Epoch: 24 Average Test Loss: 8408.1008 \n",
            "===> Epoch: 25 Average Train Loss: 8332.5711 \n",
            "===> Epoch: 25 Average Test Loss: 7701.6681 \n",
            "===> Epoch: 26 Average Train Loss: 7542.5025 \n",
            "===> Epoch: 26 Average Test Loss: 6954.1945 \n",
            "===> Epoch: 27 Average Train Loss: 6942.3608 \n",
            "===> Epoch: 27 Average Test Loss: 6539.2177 \n",
            "===> Epoch: 28 Average Train Loss: 6556.9289 \n",
            "===> Epoch: 28 Average Test Loss: 6106.2953 \n",
            "===> Epoch: 29 Average Train Loss: 6137.9983 \n",
            "===> Epoch: 29 Average Test Loss: 5964.5345 \n",
            "===> Epoch: 30 Average Train Loss: 5893.2249 \n",
            "===> Epoch: 30 Average Test Loss: 5553.9526 \n"
          ]
        }
      ]
    }
  ]
}
